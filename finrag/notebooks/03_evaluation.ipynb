{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Enterprise RAG System - Evaluation\n",
                "\n",
                "This notebook covers evaluation metrics:\n",
                "1. **Faithfulness**: Every claim traceable to context\n",
                "2. **Recall**: Relevant information retrieved\n",
                "3. **Precision**: Minimal irrelevant content\n",
                "4. **Citation Accuracy**: Correct section/page references\n",
                "5. **Failure Handling**: Proper \"not found\" responses"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "from config.settings import settings\n",
                "settings.initialize()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluation Framework\n",
                "\n",
                "### Metrics Definition\n",
                "\n",
                "| Metric | Description | Target |\n",
                "|--------|-------------|--------|\n",
                "| Faithfulness | % of claims grounded in context | >95% |\n",
                "| Retrieval Recall | % of relevant chunks retrieved | >80% |\n",
                "| Citation Accuracy | % of citations with correct page | >90% |\n",
                "| \"Not Found\" Precision | Correct identification of unanswerable | >95% |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from dataclasses import dataclass\n",
                "from typing import List, Dict, Any\n",
                "\n",
                "@dataclass\n",
                "class EvaluationResult:\n",
                "    \"\"\"Result of evaluating a single query.\"\"\"\n",
                "    query: str\n",
                "    expected_answer: str\n",
                "    actual_answer: str\n",
                "    \n",
                "    # Metrics\n",
                "    faithfulness_score: float  # 0-1\n",
                "    retrieval_recall: float    # 0-1\n",
                "    citation_accuracy: float   # 0-1\n",
                "    \n",
                "    # Flags\n",
                "    is_grounded: bool\n",
                "    has_hallucination: bool\n",
                "    correct_not_found: bool\n",
                "\n",
                "print(\"Evaluation framework defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Test Cases\n",
                "\n",
                "### Answerable Questions (should find answer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "answerable_queries = [\n",
                "    {\n",
                "        \"query\": \"What are the main risk factors mentioned in the filing?\",\n",
                "        \"expected_section\": \"Item 1A\",\n",
                "        \"expected_keywords\": [\"risk\", \"uncertainty\", \"factors\"]\n",
                "    },\n",
                "    {\n",
                "        \"query\": \"What was the total revenue for the fiscal year?\",\n",
                "        \"expected_section\": \"Item 7\",\n",
                "        \"expected_keywords\": [\"revenue\", \"sales\", \"$\"]\n",
                "    },\n",
                "    {\n",
                "        \"query\": \"Describe the company's business model.\",\n",
                "        \"expected_section\": \"Item 1\",\n",
                "        \"expected_keywords\": [\"business\", \"products\", \"services\"]\n",
                "    }\n",
                "]\n",
                "\n",
                "print(f\"Defined {len(answerable_queries)} answerable test cases\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Unanswerable Questions (should return \"not found\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "unanswerable_queries = [\n",
                "    \"What is the weather forecast for tomorrow?\",\n",
                "    \"Who won the Super Bowl last year?\",\n",
                "    \"What is the capital of France?\",\n",
                "    \"What did the CEO have for breakfast?\"\n",
                "]\n",
                "\n",
                "print(f\"Defined {len(unanswerable_queries)} unanswerable test cases\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluation Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "\n",
                "def check_faithfulness(answer: str, context_chunks: list) -> float:\n",
                "    \"\"\"\n",
                "    Check if answer claims are grounded in context.\n",
                "    \n",
                "    Simple approach: Check if key phrases in answer\n",
                "    appear in source chunks.\n",
                "    \"\"\"\n",
                "    if not context_chunks:\n",
                "        return 0.0\n",
                "    \n",
                "    # Combine context\n",
                "    context_text = \" \".join(c.text for c in context_chunks).lower()\n",
                "    \n",
                "    # Extract key phrases from answer (simplified)\n",
                "    # In production, use NLP to extract claims\n",
                "    answer_sentences = answer.split('.')\n",
                "    \n",
                "    grounded = 0\n",
                "    total = 0\n",
                "    \n",
                "    for sentence in answer_sentences:\n",
                "        sentence = sentence.strip().lower()\n",
                "        if len(sentence) < 10:\n",
                "            continue\n",
                "        \n",
                "        total += 1\n",
                "        # Check if key words appear in context\n",
                "        words = [w for w in sentence.split() if len(w) > 4]\n",
                "        matches = sum(1 for w in words if w in context_text)\n",
                "        \n",
                "        if matches / max(len(words), 1) > 0.3:\n",
                "            grounded += 1\n",
                "    \n",
                "    return grounded / max(total, 1)\n",
                "\n",
                "\n",
                "def check_is_not_found(answer: str) -> bool:\n",
                "    \"\"\"Check if answer indicates 'information not found'.\"\"\"\n",
                "    patterns = [\n",
                "        r\"not present in the provided documents\",\n",
                "        r\"information is not available\",\n",
                "        r\"cannot find\",\n",
                "        r\"no information about\"\n",
                "    ]\n",
                "    \n",
                "    answer_lower = answer.lower()\n",
                "    return any(re.search(p, answer_lower) for p in patterns)\n",
                "\n",
                "\n",
                "def check_citation_accuracy(citations: list, expected_section: str) -> float:\n",
                "    \"\"\"Check if citations reference expected section.\"\"\"\n",
                "    if not citations:\n",
                "        return 0.0\n",
                "    \n",
                "    matches = sum(\n",
                "        1 for c in citations \n",
                "        if expected_section.lower() in c.get('section', '').lower()\n",
                "    )\n",
                "    \n",
                "    return matches / len(citations)\n",
                "\n",
                "print(\"Evaluation functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.pipeline.query import QueryPipeline\n",
                "\n",
                "# Initialize pipeline\n",
                "# pipeline = QueryPipeline()\n",
                "\n",
                "def evaluate_query(query: str, expected_section: str = None):\n",
                "    \"\"\"\n",
                "    Evaluate a single query.\n",
                "    \n",
                "    Returns evaluation metrics.\n",
                "    \"\"\"\n",
                "    # Run query\n",
                "    # response = pipeline.query(query, verbose=False)\n",
                "    \n",
                "    # Evaluate\n",
                "    # faithfulness = check_faithfulness(response.answer, response.source_chunks)\n",
                "    # is_not_found = check_is_not_found(response.answer)\n",
                "    # citation_acc = check_citation_accuracy(response.citations, expected_section or '')\n",
                "    \n",
                "    # return {\n",
                "    #     'query': query,\n",
                "    #     'answer': response.answer,\n",
                "    #     'faithfulness': faithfulness,\n",
                "    #     'confidence': response.confidence,\n",
                "    #     'is_not_found': is_not_found,\n",
                "    #     'citation_accuracy': citation_acc,\n",
                "    #     'num_sources': len(response.source_chunks)\n",
                "    # }\n",
                "    \n",
                "    print(f\"Evaluation function ready (uncomment when documents ingested)\")\n",
                "\n",
                "evaluate_query(\"test\")  # Demo call"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Known Failure Cases\n",
                "\n",
                "### 1. Complex Numerical Reasoning\n",
                "The system may struggle with multi-step calculations that require combining data from multiple tables.\n",
                "\n",
                "### 2. Temporal Comparisons\n",
                "Comparisons across multiple years require understanding time context.\n",
                "\n",
                "### 3. Implicit Information\n",
                "Information that must be inferred (not explicitly stated) may not be retrieved.\n",
                "\n",
                "### 4. Very Long Documents\n",
                "Section detection may fail on non-standard PDF layouts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "failure_cases = [\n",
                "    {\n",
                "        \"type\": \"Complex Reasoning\",\n",
                "        \"example\": \"Calculate the year-over-year growth rate for each segment\",\n",
                "        \"reason\": \"Requires math on retrieved table data\"\n",
                "    },\n",
                "    {\n",
                "        \"type\": \"Cross-Document\",\n",
                "        \"example\": \"Compare Apple and Microsoft's revenue\",\n",
                "        \"reason\": \"Requires querying multiple documents\"\n",
                "    },\n",
                "    {\n",
                "        \"type\": \"Implicit\",\n",
                "        \"example\": \"Is the company in financial trouble?\",\n",
                "        \"reason\": \"Requires inference and judgment\"\n",
                "    }\n",
                "]\n",
                "\n",
                "print(\"Known failure cases:\")\n",
                "for fc in failure_cases:\n",
                "    print(f\"\\n{fc['type']}:\")\n",
                "    print(f\"  Example: {fc['example']}\")\n",
                "    print(f\"  Reason: {fc['reason']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Recommendations for Production\n",
                "\n",
                "1. **Use RAGAS or similar framework** for automated evaluation\n",
                "2. **Create ground truth dataset** for your specific documents\n",
                "3. **Monitor confidence scores** in production\n",
                "4. **Log \"not found\" responses** for analysis\n",
                "5. **A/B test** different reranker thresholds"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}