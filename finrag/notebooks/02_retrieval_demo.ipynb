{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Enterprise RAG System - Retrieval Demo\n",
                "\n",
                "This notebook demonstrates the retrieval pipeline:\n",
                "1. Query processing (normalization, expansion, entity extraction)\n",
                "2. Hybrid retrieval (Vector + BM25 + Knowledge Graph)\n",
                "3. Cross-encoder reranking (MANDATORY)\n",
                "4. Grounded generation with citations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add project root to path\n",
                "import sys\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "from config.settings import settings\n",
                "settings.initialize()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Query Processing\n",
                "\n",
                "Before retrieval, queries are:\n",
                "1. Normalized (lowercase, clean)\n",
                "2. Intent detected (factual, comparison, trend, etc.)\n",
                "3. Expanded with synonyms and domain terms\n",
                "4. Entities extracted for KG traversal"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.retrieval.query_processor import QueryProcessor\n",
                "\n",
                "processor = QueryProcessor()\n",
                "\n",
                "# Example queries\n",
                "queries = [\n",
                "    \"What are the main risk factors?\",\n",
                "    \"How did revenue change compared to last year?\",\n",
                "    \"Show me the breakdown of sales by segment\",\n",
                "    \"What is the company's total debt?\"\n",
                "]\n",
                "\n",
                "for query in queries:\n",
                "    processed = processor.process(query)\n",
                "    print(f\"\\nQuery: {query}\")\n",
                "    print(f\"  Intent: {processed.intent.value}\")\n",
                "    print(f\"  Entities: {processed.entities}\")\n",
                "    print(f\"  Domain terms: {processed.domain_terms}\")\n",
                "    print(f\"  Wants table: {processed.wants_table}\")\n",
                "    print(f\"  Expanded queries: {len(processed.expanded_queries)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Hybrid Retrieval\n",
                "\n",
                "Three parallel retrieval paths:\n",
                "1. **Vector Search** (semantic similarity) - weight 0.5\n",
                "2. **BM25 Search** (keyword matching) - weight 0.3\n",
                "3. **KG Traversal** (entity relationships) - weight 0.2\n",
                "\n",
                "Results merged using Reciprocal Rank Fusion (RRF)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.retrieval.hybrid_retriever import HybridRetriever\n",
                "\n",
                "retriever = HybridRetriever()\n",
                "\n",
                "print(\"Retriever configuration:\")\n",
                "print(f\"  Vector weight: {retriever.vector_weight}\")\n",
                "print(f\"  BM25 weight: {retriever.bm25_weight}\")\n",
                "print(f\"  KG weight: {retriever.kg_weight}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example retrieval (requires ingested documents)\n",
                "# query = \"What are the risk factors?\"\n",
                "# results = retriever.retrieve(query, top_k=10)\n",
                "# \n",
                "# print(f\"Retrieved {len(results)} candidates\")\n",
                "# for i, r in enumerate(results[:5]):\n",
                "#     print(f\"\\n[{i+1}] Score: {r.combined_score:.4f}\")\n",
                "#     print(f\"    Sources: {r.sources}\")\n",
                "#     print(f\"    Section: {r.section_title}\")\n",
                "#     print(f\"    Text: {r.text[:100]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Cross-Encoder Reranking (MANDATORY)\n",
                "\n",
                "**This is a REQUIRED component**. The system is invalid without reranking.\n",
                "\n",
                "Reranking uses a cross-encoder model that:\n",
                "1. Jointly encodes query and document\n",
                "2. Produces more accurate relevance scores\n",
                "3. Applies confidence threshold (0.3 default)\n",
                "4. Filters out low-confidence results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.retrieval.reranker import CrossEncoderReranker\n",
                "\n",
                "reranker = CrossEncoderReranker()\n",
                "\n",
                "print(\"Reranker configuration:\")\n",
                "print(f\"  Model: {reranker.model_name}\")\n",
                "print(f\"  Device: {reranker.device}\")\n",
                "print(f\"  Min score threshold: {reranker.min_score}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example reranking (requires retrieval results)\n",
                "# ranked = reranker.rerank(query, results, top_k=5)\n",
                "# \n",
                "# print(f\"{len(ranked)} chunks passed threshold\")\n",
                "# for r in ranked:\n",
                "#     print(f\"\\nScore: {r.rerank_score:.4f} (passes: {r.passes_threshold})\")\n",
                "#     print(f\"Section: {r.section_title}\")\n",
                "#     print(f\"Text: {r.text[:100]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Full Query Pipeline\n",
                "\n",
                "The complete workflow:\n",
                "1. Process query\n",
                "2. Hybrid retrieval\n",
                "3. Reranking\n",
                "4. Confidence check\n",
                "5. Context assembly\n",
                "6. Grounded generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.pipeline.query import QueryPipeline\n",
                "\n",
                "pipeline = QueryPipeline()\n",
                "\n",
                "# Example query (requires ingested documents)\n",
                "# response = pipeline.query(\n",
                "#     query=\"What are the main risk factors?\",\n",
                "#     verbose=True\n",
                "# )\n",
                "# \n",
                "# print(f\"\\n=== RESPONSE ===\")\n",
                "# print(f\"Answer: {response.answer}\")\n",
                "# print(f\"Has answer: {response.has_answer}\")\n",
                "# print(f\"Confidence: {response.confidence:.2%}\")\n",
                "# print(f\"Citations: {len(response.citations)}\")\n",
                "# print(f\"Tables: {len(response.tables)}\")\n",
                "# print(f\"Processing time: {response.processing_time_ms:.0f}ms\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Failure Cases\n",
                "\n",
                "The system handles failures gracefully:\n",
                "\n",
                "1. **No matching chunks**: Returns \"information not present\"\n",
                "2. **Low confidence**: Explicitly states uncertainty\n",
                "3. **Missing tables**: Only includes if referenced"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test with out-of-scope query\n",
                "# response = pipeline.query(\n",
                "#     query=\"What is the weather today?\",\n",
                "#     verbose=True\n",
                "# )\n",
                "# \n",
                "# print(f\"Answer: {response.answer}\")\n",
                "# print(f\"Has answer: {response.has_answer}\")  # Should be False"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Key Design Decisions\n",
                "\n",
                "### Why Hybrid Retrieval?\n",
                "- Vector: Semantic similarity (concept matching)\n",
                "- BM25: Exact keywords (rare terms, numbers)\n",
                "- KG: Entity relationships (connected concepts)\n",
                "\n",
                "### Why Reranking is MANDATORY?\n",
                "- Bi-encoders are approximate\n",
                "- Cross-encoders are more accurate\n",
                "- Confidence thresholds prevent hallucination\n",
                "\n",
                "### Why Confidence Thresholds?\n",
                "- Zero hallucination tolerance\n",
                "- Better to say \"not found\" than guess\n",
                "- Configurable based on use case"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}