{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enterprise RAG System - Ingestion Demo\n",
    "\n",
    "This notebook demonstrates the document ingestion pipeline:\n",
    "1. PDF parsing with structure extraction\n",
    "2. Table extraction (never vectorized)\n",
    "3. Hybrid chunking (single canonical pass)\n",
    "4. Multi-store population (FAISS, Neo4j, SQLite, BM25)\n",
    "\n",
    "All stores use the SAME chunks with SAME chunk_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# Initialize settings\n",
    "from config.settings import settings\n",
    "settings.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PDF Parsing\n",
    "\n",
    "The parser extracts:\n",
    "- Document structure (SEC Items, Parts, Sections)\n",
    "- Text content with page numbers\n",
    "- Tables as separate elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.document.parser import PDFParser\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize parser\n",
    "parser = PDFParser()\n",
    "\n",
    "# Parse a sample PDF (replace with your file)\n",
    "# pdf_path = Path('../data/uploads/sample_10k.pdf')\n",
    "# document = parser.parse(pdf_path)\n",
    "\n",
    "# For demo, we'll show the structure\n",
    "print(\"Parser initialized\")\n",
    "print(f\"Detects SEC Items, Parts, and section structure\")\n",
    "print(f\"Tracks page numbers for citations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hybrid Chunking\n",
    "\n",
    "Single canonical chunking pass that produces chunks used by ALL stores.\n",
    "\n",
    "Strategy:\n",
    "1. Structural boundaries (SEC Items, Parts)\n",
    "2. Semantic boundaries (paragraphs, concepts)\n",
    "3. Token limits with overlap\n",
    "4. Table placeholders (tables never embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.document.chunker import HybridChunker\n",
    "\n",
    "chunker = HybridChunker(\n",
    "    max_tokens=512,\n",
    "    min_tokens=50,\n",
    "    overlap_tokens=64\n",
    ")\n",
    "\n",
    "print(\"Chunker configuration:\")\n",
    "print(f\"  Max tokens: {chunker.max_tokens}\")\n",
    "print(f\"  Min tokens: {chunker.min_tokens}\")\n",
    "print(f\"  Overlap: {chunker.overlap_tokens}\")\n",
    "print(f\"  Table placeholder format: {chunker.table_placeholder_format}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Table Extraction\n",
    "\n",
    "Tables are NEVER vectorized. They are:\n",
    "1. Extracted to structured JSON\n",
    "2. Stored in SQLite\n",
    "3. Linked to explanatory chunks via table_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.document.table_extractor import TableExtractor\n",
    "\n",
    "extractor = TableExtractor()\n",
    "\n",
    "print(\"Table extractor features:\")\n",
    "print(\"  - Column type inference (text, number, currency, percentage)\")\n",
    "print(\"  - Schema generation for SQL\")\n",
    "print(\"  - Markdown formatting for LLM\")\n",
    "print(\"  - Link to context chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Full Ingestion Pipeline\n",
    "\n",
    "Orchestrates all steps:\n",
    "1. Parse → Chunk → Extract Tables\n",
    "2. Populate Vector Store (FAISS)\n",
    "3. Populate Knowledge Graph (Neo4j)\n",
    "4. Populate Table Store (SQLite)\n",
    "5. Build BM25 Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipeline.ingestion import IngestionPipeline\n",
    "\n",
    "pipeline = IngestionPipeline()\n",
    "\n",
    "print(\"Ingestion pipeline initialized\")\n",
    "print(f\"  Vector store: {type(pipeline.vector_store).__name__}\")\n",
    "print(f\"  BM25 index: {type(pipeline.bm25_index).__name__}\")\n",
    "print(f\"  Table store: {type(pipeline.table_store).__name__}\")\n",
    "print(f\"  Knowledge graph: {type(pipeline.knowledge_graph).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example ingestion (uncomment with real file)\n",
    "# result = pipeline.ingest('../data/uploads/sample_10k.pdf')\n",
    "# \n",
    "# print(f\"Document ID: {result.doc_id}\")\n",
    "# print(f\"Title: {result.title}\")\n",
    "# print(f\"Pages: {result.page_count}\")\n",
    "# print(f\"Chunks created: {result.chunks_created}\")\n",
    "# print(f\"Tables extracted: {result.tables_extracted}\")\n",
    "# print(f\"Entities extracted: {result.entities_extracted}\")\n",
    "# print(f\"Processing time: {result.processing_time_seconds:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Store Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check store counts\n",
    "print(\"Store statistics:\")\n",
    "print(f\"  Vector store: {pipeline.vector_store.count} vectors\")\n",
    "print(f\"  BM25 index: {pipeline.bm25_index.count} chunks\")\n",
    "print(f\"  Table store: {pipeline.table_store.count} tables\")\n",
    "print(f\"  Knowledge graph: {pipeline.knowledge_graph.node_count} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Design Decisions\n",
    "\n",
    "### Why Single Canonical Chunking?\n",
    "- Ensures consistency across all stores\n",
    "- Every store references the SAME chunk_id\n",
    "- Enables cross-store deduplication\n",
    "\n",
    "### Why Tables are NOT Vectorized?\n",
    "- Embeddings don't capture tabular structure\n",
    "- Numerical precision lost in embedding\n",
    "- SQL enables precise queries\n",
    "- LLMs understand structured tables better\n",
    "\n",
    "### Why Neo4j is MANDATORY?\n",
    "- Captures entity relationships\n",
    "- Enables graph-based retrieval\n",
    "- Document structure representation\n",
    "- Cross-document knowledge linking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
